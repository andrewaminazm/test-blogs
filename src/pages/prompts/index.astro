---
import BaseLayout from '../../layouts/BaseLayout.astro';
---

<BaseLayout title="Prompt library — AI × Testing" description="Copy-paste prompts for test-case generation, review, and maintenance.">
  <div class="page">
    <h1>Prompt library</h1>
    <p class="lead">Prompts you can paste into ChatGPT, Claude, or Copilot to generate test cases, review tests, and more.</p>

    <section class="prompt-list">
      <article class="prompt-card">
        <h2>Test scenarios from user story</h2>
        <p>Give the model a user story and get structured test scenarios (happy path, validation, edge cases).</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Given this user story:
[Paste user story here]

Generate 5–7 test scenarios. For each scenario provide:
1. Scenario name
2. Preconditions
3. Steps
4. Expected result
5. Type: happy path | validation | edge case | security

Output as a markdown table.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>API test cases from OpenAPI/Swagger</h2>
        <p>Turn an API spec into concrete test cases (status codes, payloads, error handling).</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Here is our API specification (OpenAPI/Swagger):
[Paste spec or link]

For each endpoint, generate test cases covering:
- 200/201 success
- 400 validation errors
- 401/403 auth
- 404 not found
- One negative/edge case per endpoint

Format: endpoint | method | scenario | expected status | sample payload (if needed).</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Review AI-generated tests</h2>
        <p>Use AI to critique and improve tests that were themselves AI-generated.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Review these automated test cases for correctness and maintainability:

[Paste test code]

List:
1. Logic bugs or false assumptions
2. Flaky patterns (e.g. arbitrary waits, brittle selectors)
3. Missing assertions or edge cases
4. Suggested improvements with code snippets</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Exploratory test charter</h2>
        <p>Get a focused mission and focus areas for a time-boxed exploratory session.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're testing [feature or screen name]. Context: [1–2 sentences].

Create an exploratory test charter with:
- Mission (one sentence)
- 5 areas to focus on (specific behaviors or risks)
- 3 test data ideas (e.g. "user with no history", "max length input")
- 2 things to avoid (e.g. "don't assume backend is correct")

Keep each item to one line. Output as a simple list.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>API test payload variants</h2>
        <p>Generate minimal, full, invalid, and edge-case request bodies from a schema or sample.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We need test request bodies for our REST API. Schema or sample:

[Paste JSON schema or a sample valid payload]

Generate 5 variants:
1. Minimal valid (only required fields)
2. Full valid (all optional fields filled)
3. Invalid: missing required field "X"
4. Invalid: wrong type for field "Y"
5. Edge case: empty string where allowed, or max length

Output as JSON array. Use clearly fake data (test@example.com, "Test User").</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Bug report from failure</h2>
        <p>Turn a failing test or error message into a concise bug report draft.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Turn this test failure into a short bug report:

[Paste error output, stack trace, or failing assertion]

Include:
- Title (one line)
- Steps to reproduce (numbered)
- Expected vs actual
- Environment (browser/API version if relevant)
- Severity suggestion (critical/major/minor) with one-line reason</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Performance: load test scenario design</h2>
        <p>Get scenarios, user mix, think times, and metrics to track for a load or stress test.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're planning a load test for our app. [1–2 sentences: main flows, e.g. login, browse, checkout.]

Provide:
1. 4–5 performance scenarios (name + short steps)
2. Suggested user mix (e.g. 70% browsing, 20% cart, 10% checkout)
3. Think time between actions (min–max in seconds)
4. Which 2–3 endpoints or pages we should monitor for p95 latency
5. One stress scenario idea (spike or sustained high load)

Assume [e.g. 50–100] virtual users, [e.g. 10] minute run. Output as a table or structured list.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Performance: interpret load test results</h2>
        <p>Paste a short summary of metrics and get hypotheses for bottlenecks or regressions.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We ran a load test. Context: [tool, duration, virtual users, main flow].

Results summary:
[Paste p50/p95/p99, error rate, CPU or key metrics—keep it short]

What might explain the main bottleneck or the spike in [metric]? Suggest 2–3 things to check next (e.g. DB, cache, connection pool, GC). One line per suggestion.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Load test plan for JMeter</h2>
        <p>Get a JMeter test plan outline: thread groups, ramp-up, duration, samplers, and listeners.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're designing a JMeter load test. Context: [e.g. REST API / web app], main flows: [e.g. login, get list, submit form].

Provide a JMeter test plan structure:
1. Thread Group: number of threads (users), ramp-up period (seconds), loop count or duration
2. For each flow: which sampler (HTTP Request / etc.), method, path, and any body or CSV data
3. Suggested timers: Constant Timer think time between requests (ms)
4. Listeners to add: Summary Report, View Results Tree (for debug), and one for response time (e.g. Response Time Graph or Aggregate Report)
5. Assertions: what to check (e.g. response code 200, response time &lt; 500 ms)

Output as a numbered checklist or table. Assume we'll create the JMX in JMeter GUI or paste HTTP Request defaults.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Create JMX file structure for JMeter</h2>
        <p>Get step-by-step instructions or XML-style outline to build a JMeter test plan (JMX).</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We need to create a JMeter JMX test plan for this scenario:

[Describe: e.g. 50 users over 2 min ramp-up, run for 5 min. Three HTTP requests: GET /api/products, GET /api/product/&#123;id&#125;, POST /api/cart with JSON body. Think time 1–3 s between requests.]

Output:
1. Exact structure: Test Plan → Thread Group → (Config Element if needed) → Samplers → Timers → Listeners
2. For each HTTP Request: name, method, server/path, body (if POST). Note where to use variables (e.g. $&#123;id&#125; from CSV or extractor)
3. Suggested values: Thread Group (threads, ramp-up, duration), Constant Timer (ms), any CSV Data Set Config columns
4. If you can output valid JMeter JMX XML snippet for one Thread Group with one HTTP Request, include it; otherwise give clear steps to build it in JMeter GUI so we can save as JMX.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Performance testing with Postman</h2>
        <p>Plan a Postman-based performance run: collection runner, Newman, or Postman load/flow and what to measure.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We want to run performance tests using Postman. We have a collection with [e.g. login, get list, create item] requests.

Provide:
1. Order of requests for a realistic flow (e.g. login first, then use token in subsequent requests)
2. How to run for load: Postman Collection Runner (iterations, delay) vs Newman in CLI with --iteration-count and --delay-request, or Postman Flows for sequence. Which is better for [e.g. 100 iterations, 10 concurrent]?
3. What to measure: response time per request, failed requests, and how to export or report (e.g. Newman with --reporters html, or Postman summary)
4. Optional: one example Newman command for 50 iterations with 500 ms delay and HTML report
5. Limits of Postman/Newman for "real" load (e.g. single process); when to use JMeter or k6 instead for high concurrency</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>UI test draft from flow description</h2>
        <p>Describe a user flow in plain language and get a Playwright (or similar) test draft with stable selectors.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're writing a UI test in Playwright. Flow:

[E.g. User goes to /login, fills email and password, clicks "Sign in", and should see the dashboard with "Welcome" text.]

Generate a single test that:
1. Goes to the page
2. Fills the fields (use getByRole or getByLabel only)
3. Clicks submit
4. Asserts the expected outcome (URL or visible text)

Use stable selectors only (no class names or brittle IDs). Add a short comment per step.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Visual regression: prioritize screens</h2>
        <p>Get a shortlist of high-value screens or components to add visual regression tests first.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're adding visual regression tests. Our app: [1–2 sentences: type of app, main areas].

List 5 high-value screens or components to add visual tests first. For each give:
- Name or route/component
- Why it's high value (e.g. revenue, frequent change, compliance)
- One risk if it regresses (e.g. "checkout layout breaks")

Output as a short numbered list.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">Jira &amp; test management</h3>

      <article class="prompt-card">
        <h2>Story to acceptance criteria (for Jira)</h2>
        <p>Turn a rough idea into a user story and testable acceptance criteria you can paste into Jira.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We use Jira. Turn this into one user story with acceptance criteria:

[Paste product idea or summary]

Output:
- Title: "As a [role], I want [goal] so that [benefit]"
- 3–5 acceptance criteria, each testable (Given/When/Then or "System shall...")
- 1–2 edge cases to consider

Format for Jira description. No markdown headers in the criteria list.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Jira acceptance criteria → test cases</h2>
        <p>Generate test case summaries and steps from Jira acceptance criteria for bulk creation.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>This Jira story has these acceptance criteria:

[Paste AC from Jira]

Generate test cases for Jira. For each: Summary (Jira title) | Steps (numbered) | Expected result | AC covered (e.g. AC2). Output as table. We'll create one Jira test issue per row.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Sprint test planning (Jira)</h2>
        <p>From a list of story titles, get suggested AC and test case summaries for the sprint.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Next sprint Jira story titles: [paste 5–10 titles].

For each: 2–3 acceptance criteria (short) and 1–2 test case summaries. Output: Story title | AC | Test summary. Table format for pasting into Jira.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">Security testing</h3>

      <article class="prompt-card">
        <h2>Security test cases from feature description</h2>
        <p>Get auth, injection, and logic abuse test cases with severity and example payload ideas.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Feature/API: [short description]. Generate security test cases for:

1. Auth: unauthorized access, privilege escalation, session (2–3 each)
2. Input: SQLi, XSS, path traversal (1–2 each, with example payload idea)
3. Business logic: e.g. bypass payment, change another user's data (2)
4. Info disclosure: error messages, debug endpoints (1–2)

Output: test name | steps | expected (secure) behavior | severity. Table.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Prompt injection test inputs</h2>
        <p>Generate inputs to test LLM-powered features for prompt injection and jailbreaks.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have a [e.g. customer-support] chatbot. Generate 10 prompt-injection style inputs that might try to:
- Override system instructions
- Reveal system prompt or other users' data
- Force output in a different format or role

Output as a numbered list. We'll run these in test and check the bot refuses or stays in character.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>SAST/DAST finding triage</h2>
        <p>Summarize a security finding and get a fix or priority suggestion. Use anonymized snippets only.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Security finding (anonymized): [paste rule, file/endpoint, short context—no secrets].

Is this likely a true positive? One-line fix or mitigation. Severity suggestion (critical/high/medium/low) with one-line reason. Do not paste full source.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">API testing (more)</h3>

      <article class="prompt-card">
        <h2>API contract test matrix from OpenAPI</h2>
        <p>Get a full scenario matrix: status codes, request variants, and which response fields to assert.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>OpenAPI path: [paste path + request body schema + response codes].

Generate contract-style test matrix:
- Per documented status (200, 400, 401, 404): one test with request that triggers it
- Missing required field → 400; wrong type → 400; valid minimal → 200 + list key response assertions

Output: scenario | request (key fields) | expected status | assertions. Table.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>API negative / fuzz payloads</h2>
        <p>Generate invalid and edge-case request bodies to stress validation and error handling.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Request body schema or sample: [paste].

Generate 5 negative cases: missing required field, wrong type, empty string for number, null for required, value over max length. For each: short description + example JSON. Then 3 fuzz-style: huge string, unicode, empty object.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>API seed data design</h2>
        <p>Get minimal seed entities (users, orders, etc.) for API tests. You implement the seed script.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Our API tests need: [e.g. 1 admin, 2 users, 3 orders in different states]. We create via API/DB.

Suggest minimal seed data (fields + example values) in creation order. Output as table or JSON. Use fake data only (test@example.com).</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>API change impact on tests</h2>
        <p>From a list of API changes, get which tests might break and what to update.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>API changed: [list new/removed/changed fields or endpoints].

Our tests: [short description, e.g. "one file per endpoint, 200/400/401/404"]. Which tests might break? What to add (e.g. new assertions), remove, or update? Bullet list.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">Test planning &amp; management</h3>

      <article class="prompt-card">
        <h2>Test plan outline</h2>
        <p>Get a test plan structure: scope, approach, schedule, resources, risks, and exit criteria.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're writing a test plan for [product/release in 1 sentence]. Key features: [list 3–5].

Provide a test plan outline with sections:
1. Scope (in scope / out of scope)
2. Test approach (functional, regression, API, performance—what we'll do)
3. Test types and deliverables (test cases, automation, reports)
4. Schedule (phases or sprint alignment)
5. Risks and mitigations (2–3)
6. Exit criteria (when we sign off)
7. Environment and test data needs

One paragraph or bullet list per section. No fluff.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Regression test selection / prioritization</h2>
        <p>Decide which tests to run in regression based on changes and risk.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>This release changed: [list modules, APIs, or features]. Our full suite has [e.g. 500] tests.

Suggest how to select tests for regression:
1. Must-run (critical path, changed area, payment/auth)—list categories
2. Should-run (related modules, integration points)
3. Can-skip if time-boxed (low-risk, stable areas)
4. Suggested test execution order (smoke first, then critical, then rest)

Output as a short prioritization guide. Assume we have 2 hours for regression; suggest a 30-min smoke subset.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Smoke test suite design</h2>
        <p>Define a minimal smoke suite to verify the build is testable.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Our app: [1–2 sentences]. Main entry points: [e.g. login, home, one critical flow].

Design a smoke test suite (5–10 tests max) that:
- Verifies the app starts and core services respond
- Covers one happy path per critical area (login, main API, main screen)
- Can run in under 10 minutes
- Is stable (no flaky steps)

Output: test name | steps (1–3 each) | expected. Table or list.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Release / go-live checklist</h2>
        <p>Get a pre-release checklist: testing, config, rollback, and sign-off.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're about to release [product/release]. Environment: [e.g. web + API, DB].

Generate a go-live checklist:
1. Testing: smoke, regression, critical path—what must be green
2. Config: feature flags, env vars, DB migrations—what to verify
3. Rollback: steps and data backup
4. Sign-off: who approves (dev, QA, ops) and what they check
5. Post-release: monitoring, quick sanity—first 15 min

Output as a checklist (checkbox style). One line per item.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Test estimation</h2>
        <p>Estimate test effort for stories or a release.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We need to estimate testing for [sprint/release]. Scope: [list 5–10 user stories or features in one line each].

For each item suggest:
- Test design time (hours)
- Test execution time (hours, assume 1–2 runs)
- Automation effort if applicable (hours, or "manual only")
- Dependencies or risks that could change the estimate

Output as table: Story/Feature | Design | Execution | Automation | Notes. Then give total and buffer suggestion (e.g. +20%).</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Exit criteria definition</h2>
        <p>Define when testing is done and release is acceptable.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Project: [1 sentence]. We need clear exit criteria for QA sign-off.

Suggest 5–7 exit criteria covering:
- Test execution (e.g. all critical tests passed, no P1/P2 open)
- Coverage (e.g. all AC covered, critical paths automated)
- Quality (e.g. defect rate, no known critical bugs)
- Non-functional (e.g. performance budget, security scan passed)
- Documentation and handover

Each criterion: one line, measurable where possible (e.g. "&lt; 2 P2 bugs open").</code></pre>
        </div>
      </article>

      <h3 class="section-heading">BRD &amp; requirements</h3>

      <article class="prompt-card">
        <h2>BRD structure and outline</h2>
        <p>Get a Business Requirements Document (BRD) outline so requirements are clear and testable.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're writing a BRD for [project/product in 1 sentence]. Key stakeholders: [e.g. business, ops, IT]. Scope: [2–3 bullet points].

Suggest a BRD structure with sections:
1. Executive summary (purpose, scope, high-level outcome)
2. Business objectives and success criteria (measurable)
3. Stakeholders and users (roles, who does what)
4. Functional requirements (numbered, one per requirement; format: "The system shall...")
5. Non-functional requirements (performance, security, availability—one line each)
6. Assumptions and constraints
7. Out of scope
8. Glossary (key terms)

For section 4, give 3–5 example requirement statements we can expand. Each requirement must be testable.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>BRD to test scenarios and traceability</h2>
        <p>Turn BRD requirements into test scenarios and a traceability matrix.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>BRD requirements (numbered): [paste 5–15 "The system shall..." or similar].

For each requirement:
1. Generate 1–3 test scenarios (scenario name, steps, expected result)
2. Suggest test type (functional, API, UI, integration) and priority (P1/P2/P3)

Output as traceability table: Req ID | Requirement (short) | Test scenario | Steps summary | Type | Priority. So we can map tests back to BRD and report coverage.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>BRD review for testability</h2>
        <p>Review BRD sections and flag vague or untestable requirements.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have a BRD section or list of requirements: [paste].

Review for testability:
1. List requirements that are vague (e.g. "user-friendly", "fast")—suggest how to make them testable (e.g. "load time &lt; 2s", "WCAG AA")
2. List requirements that are missing (e.g. error handling, edge cases, security)
3. For each requirement: is it testable as-is? If not, suggest a clearer version in "The system shall..." format
4. Suggest 2–3 acceptance criteria for the most critical requirement

Output as table: Req/Line | Issue (vague/missing/untestable) | Suggested fix.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>BRD to acceptance criteria</h2>
        <p>Convert BRD requirements into acceptance criteria for stories or test cases.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>BRD requirement: [paste one or more "The system shall..." statements].

Convert each into 2–4 acceptance criteria that a tester or product owner can sign off. Use format: "Given [context], When [action], Then [observable result]" or "The system shall [specific, measurable behavior]." Each AC must be pass/fail testable. Output as numbered list.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>BRD gap analysis (requirements vs scope)</h2>
        <p>Check if BRD covers the scope and spot gaps or conflicts.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Project scope or epic summary: [paste]. BRD requirements (numbered): [paste].

Perform a gap analysis:
1. Scope items not covered by any requirement—list and suggest a requirement
2. Requirements that don't align with stated scope—flag and suggest (drop or move to later)
3. Overlaps or conflicts between requirements—list and suggest merge or clarify
4. Missing NFRs (e.g. performance, security, compliance) for this scope—suggest 2–3

Output as short sections with bullet lists. One paragraph summary at the end.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">BDD &amp; test design</h3>

      <article class="prompt-card">
        <h2>BDD / Gherkin scenarios</h2>
        <p>Turn a user story or flow into Given/When/Then scenarios.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>User story or flow: [paste].

Generate 3–5 BDD scenarios in Gherkin format (Feature, Scenario, Given, When, Then). Include:
- Happy path
- 1–2 validation or error scenarios
- One edge case

Use consistent wording. No implementation details in steps (e.g. "When the user submits the form" not "When the user clicks button#submit").</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Boundary value &amp; equivalence partitioning</h2>
        <p>Get boundary and equivalence class test cases for inputs.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have these inputs to test: [e.g. age 1–120, quantity 0–999, email string max 255 chars].

For each input:
1. Boundary values (min, min-1, max, max+1, zero/empty if applicable)
2. Equivalence classes (valid range, invalid low, invalid high, wrong type)
3. One test case per boundary/class with expected result

Output as table: Input | Test value | Class/Boundary | Expected. Keep to 10–15 test cases total.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Negative test case generation</h2>
        <p>Generate negative test cases from a feature or API description.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Feature or API: [short description]. We need negative test cases.

Generate 5–8 negative test cases:
- Invalid input (wrong type, out of range, malformed)
- Missing required data
- Unauthorized or wrong role
- Invalid state or sequence (e.g. action not allowed in current state)
- One "abuse" or unexpected usage

For each: scenario name | steps/data | expected (error message or code). Table.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">UAT &amp; acceptance</h3>

      <article class="prompt-card">
        <h2>UAT scenarios from user stories</h2>
        <p>Turn user stories into UAT scenarios for business users.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>User stories for this release: [paste 3–5 stories with acceptance criteria].

Generate UAT scenarios that a business user can execute and sign off. For each:
- Scenario title (business language)
- Preconditions
- Steps (user actions only, no technical jargon)
- Expected result
- Sign-off checkbox idea (e.g. "I confirm orders display correctly")

Output as table or formatted list. Max 2 pages when printed.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Acceptance criteria checklist</h2>
        <p>Convert acceptance criteria into a testable checklist.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Acceptance criteria: [paste].

Convert each AC into one or more testable checklist items. Each item: one sentence, testable (pass/fail), no ambiguity. If an AC is too vague, suggest a clearer version. Output as numbered checklist.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">Mobile &amp; compatibility</h3>

      <article class="prompt-card">
        <h2>Mobile test cases (iOS / Android)</h2>
        <p>Get mobile-specific test scenarios: gestures, permissions, interruptions, network.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Our mobile app: [1–2 sentences]. Platform: [iOS, Android, or both].

Generate mobile test cases covering:
1. Core flow (login, main feature)—3 cases
2. Gestures (swipe, pull-to-refresh, back)—2 cases
3. Permissions (camera, location, notifications)—accept, deny, revoke
4. Interruptions (call, notification, app backgrounded mid-flow)—2 cases
5. Network (offline, slow, switch WiFi to cellular)—2 cases
6. Orientation and keyboard (if applicable)

Output: scenario | steps | expected. Table.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Compatibility matrix (browsers, devices, OS)</h2>
        <p>Suggest what to test on for coverage vs effort.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have [web app / mobile app / both]. Target users: [e.g. global, mostly Chrome and Safari].

Suggest a compatibility test matrix:
1. Tier 1 (must test every release): browsers/devices/OS with % share or rationale
2. Tier 2 (test major releases): list
3. Tier 3 (spot-check or on demand): list
4. What to test per tier (e.g. smoke only vs full regression)
5. Suggested tools (BrowserStack, Sauce Labs, real devices)—one line

Output as table or list. Keep Tier 1 to 3–5 combinations.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Accessibility (a11y) test cases</h2>
        <p>Get accessibility test scenarios and WCAG-related checks.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We need to add accessibility testing for [e.g. main flows / checkout / dashboard].

Generate a11y test cases covering:
1. Keyboard navigation (tab order, focus visible, no traps)
2. Screen reader (labels, landmarks, announcements)—what to check
3. Color contrast and text resize
4. Forms (labels, errors, required)
5. One or two WCAG 2.1 Level A/AA criteria with a simple check

Output: area | check | how to verify (tool or manual). Table. Suggest axe or similar for automation.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">Database &amp; integration</h3>

      <article class="prompt-card">
        <h2>Database test cases</h2>
        <p>Design tests for data integrity, constraints, and migrations.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have [e.g. PostgreSQL / MySQL]. Key tables: [list 3–5 or describe schema briefly]. We do [CRUD, reports, migrations].

Suggest database test cases for:
1. Constraints (PK, FK, unique, not null)—what to try to break
2. Data integrity (valid values, cascades)
3. Migration (up/down, data backfill)—what to verify before/after
4. Concurrency (e.g. two users updating same row)—one scenario
5. Indexes and performance (optional)—what to measure

Output: test name | steps/query idea | expected. Table. No production data.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Integration test scenarios</h2>
        <p>Define integration tests between modules or services.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Our system: [e.g. frontend, API, payment service, notification service]. Key integrations: [list].

Suggest integration test scenarios:
1. Happy path: request flows through all layers, response correct
2. One service down or slow—expected error or fallback
3. Invalid data from external service—how we handle it
4. Contract: API request/response matches what consumer expects
5. Idempotency or retry (if applicable)—one scenario

Output: scenario | components involved | steps | expected. Table.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>E2E flow design</h2>
        <p>Design end-to-end test flows from user journey to system.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>User journey: [e.g. user signs up, verifies email, completes profile, makes first purchase].

Our stack: [e.g. web UI, API, DB, email service]. Design 2–3 E2E test flows that:
- Start from UI (or API if no UI for step)
- Cover the full journey with realistic data
- Include one failure path (e.g. payment declined) and recovery
- Note which systems must be up and any test data needed

Output: flow name | steps (high level) | systems touched | test data. Numbered steps.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>SQL and data validation tests</h2>
        <p>Design SQL-based tests to validate data after operations or migrations.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>DB: [e.g. PostgreSQL / MySQL / SQL Server]. After [e.g. order placement / user signup / nightly job] we need to verify data.

Suggest 5–7 validation checks as SQL or steps:
1. Row counts (e.g. orders table increased by 1)
2. Referential integrity (no orphaned child rows, FK valid)
3. Calculated fields (totals, status derived correctly)
4. Nulls and defaults (required columns populated)
5. One check for duplicate or invalid state
6. Optional: query performance (index used, no full scan)

Output: check name | SQL or step | expected. Use table names in square brackets if needed. No production data.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Database migration testing</h2>
        <p>Plan tests for schema migrations and data backfills.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're running a DB migration: [describe: e.g. add column, new table, backfill from old table, drop column]. Tool: [e.g. Flyway, Liquibase, raw SQL].

Suggest a migration test plan:
1. Pre-migration: backup, row counts, sample data hash (for comparison)
2. Migration run: up success, rollback/down works
3. Post-migration: schema correct (columns, types, constraints), row counts same or as expected, backfilled data correct
4. Application: app starts, one read and one write to changed area
5. Rollback scenario: if we roll back, what to verify

Output as numbered checklist. No production; use staging or copy.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Stored procedures and triggers testing</h2>
        <p>Get test cases for stored procedures, functions, and triggers.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have [stored procedures / triggers / functions] in [e.g. PostgreSQL / SQL Server]. Purpose: [e.g. "on order insert, update inventory and log"].

Suggest test cases:
1. Happy path: valid input, expected side effects (rows inserted/updated, log written)
2. Invalid input: null, out of range, duplicate—expected error or behavior
3. Trigger: before/after state of table, other tables affected
4. Concurrency: two calls at once—expected result (e.g. lock, serialized)
5. One edge case (e.g. empty set, zero, boundary value)

Output: test name | input/setup | expected (return value and DB state). Table.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Data comparison and reconciliation</h2>
        <p>Design tests to compare data across systems or before/after.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We need to reconcile or compare: [e.g. source DB vs warehouse, API response vs DB, file import vs table].

Suggest a comparison test approach:
1. Key columns to match (PK, business keys)
2. What to do with mismatches (count, list IDs, diff sample)
3. Order and null handling (sort by key, treat nulls)
4. Volume: full vs sample (e.g. last 24h), performance tip
5. One simple query or pseudocode for "count matches vs total"
6. Optional: tool (e.g. dbt test, custom script, SQL EXCEPT)

Output as short sections. No production data.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">Defects &amp; reporting</h3>

      <article class="prompt-card">
        <h2>Defect severity and priority</h2>
        <p>Get severity/priority suggestion and reproduction steps from a bug description.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Bug description: [paste summary and steps].

Suggest:
1. Severity (critical/major/minor/cosmetic) with one-line reason
2. Priority (P1/P2/P3) for our release—one line
3. Improved title (one line, clear and searchable)
4. Missing info: what else to add (browser, env, logs) for dev to fix
5. Suggested labels or component (e.g. "checkout", "API")</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Test summary report from results</h2>
        <p>Turn raw test results into a short summary report for stakeholders.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Test execution results: [paste counts: passed, failed, skipped, blocked; or paste a short summary].

Generate a 5–7 line test summary report for stakeholders:
- Scope (what was tested)
- Result (pass/fail rate, trend if applicable)
- Defects (count by severity, any blocking)
- Risks or open items
- Recommendation (ready for release / not ready / with conditions)

Tone: factual, no jargon. Use bullet points.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Flaky test analysis</h2>
        <p>Get possible causes and fixes for a flaky test from failure details.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Our test sometimes fails: [paste test name, framework, and failure message or stack trace]. It passes on retry.

Suggest:
1. Likely causes (timing, order, env, shared state, network)—rank by probability
2. Quick fixes to try (e.g. explicit wait, isolate data, retry once)
3. One code or config change with example (pseudocode or snippet)
4. How to prevent similar flakiness (pattern or rule)

Keep to one page. No generic advice only.</code></pre>
        </div>
      </article>

      <h3 class="section-heading">CI/CD &amp; test data</h3>

      <article class="prompt-card">
        <h2>CI test pipeline design</h2>
        <p>Suggest stages and jobs for running tests in CI.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We use [e.g. Jenkins / GitHub Actions / Azure DevOps]. We have: unit tests, API tests, UI tests (Playwright), and a smoke suite. Build takes ~5 min.

Suggest a CI test pipeline:
1. Stages (e.g. build → unit → API → UI smoke → full UI) and when each runs (every commit vs nightly)
2. Parallelization (what can run in parallel)
3. Failure handling (fail fast vs run all, notifications)
4. Test reports and artifacts (where to store, what to publish)
5. One example config snippet (YAML or equivalent) for one stage

Output as numbered list plus one snippet. Adapt to our tool.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Test data strategy</h2>
        <p>Define how to create, manage, and clean test data.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Our app needs: [e.g. users, orders, products, subscriptions]. We have [dev, staging, shared CI] environments.

Suggest a test data strategy:
1. Source of truth (seed scripts, API, DB copy, synthetic)—per env
2. Isolation (per test vs shared, how to avoid collisions)
3. Sensitive data (PII)—how to anonymize or generate
4. Cleanup (after run, daily, or never) and who owns it
5. One practical approach for CI (e.g. "run seed before suite, truncate after") with pros/cons

Output as short sections. No production data.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Environment setup checklist</h2>
        <p>Get a checklist for setting up a test environment.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We need to set up a [e.g. staging / QA] environment for [web app + API + DB]. We use [e.g. Docker, cloud, on-prem].

Generate an environment setup checklist:
1. Infrastructure (servers, containers, URLs)
2. Application (build, config, env vars, feature flags)
3. Data (DB, migrations, seed data)
4. External services (mocks, stubs, or real—which)
5. Smoke verification (3–5 checks that env is ready for testing)
6. Access and credentials (who gets what, where stored)

One line per item. Checkbox style.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Test coverage gap analysis</h2>
        <p>Identify gaps from a list of features and existing tests.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Features or modules: [list]. Our current tests cover: [list what you have].

Identify:
1. Gaps (features or flows with no or weak coverage)
2. Overlap (redundant tests to consider removing or merging)
3. Priority order to add tests (by risk and effort)
4. Suggested test type per gap (unit, API, UI, manual)

Output as table: Feature/Area | Coverage | Gap? | Suggested action. One paragraph summary at the end.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>GitHub Actions test workflow</h2>
        <p>Get a YAML workflow for running tests on push/PR.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We use GitHub Actions. We have: [e.g. Node 20, npm install, npm run test (unit), npm run test:api, npm run test:e2e]. We want tests to run on push to main and on every PR.

Generate a .github/workflows/test.yml that:
1. Triggers on push to main and pull_request to main
2. One job: checkout, setup Node, install deps, run unit tests
3. Second job: run API tests (can depend on job 1)
4. Optional: third job for E2E (e.g. Playwright) or matrix for multiple Node versions
5. Upload test results or report artifact if our runner produces them (e.g. junit, coverage)
6. Fail the workflow if any test fails

Output the full YAML. Use standard actions (actions/checkout, actions/setup-node).</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Jenkins pipeline for tests</h2>
        <p>Get a Jenkinsfile or pipeline stages for build and test.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We use Jenkins. Build: [e.g. Maven / npm]. We have unit tests, API tests, and UI tests (e.g. Selenium/Playwright). We want to run on every commit and keep logs and reports.

Suggest a declarative pipeline (Jenkinsfile) with stages:
1. Checkout
2. Build (compile / npm install)
3. Unit tests (with report archiving, e.g. JUnit)
4. API tests (parallel or sequential)
5. UI tests (optional, or only on main branch)
6. Post: archive artifacts, publish test results, cleanup
7. Failure: notify (e.g. Slack/email), fail the build

Include one example stage with sh step and report publishing. Mention where to set credentials (e.g. env vars).</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>CI pipeline failure triage</h2>
        <p>From a failing CI log or summary, get likely cause and next steps.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Our CI pipeline failed. Context: [e.g. GitHub Actions, stage that failed]. Summary or log excerpt:

[Paste error message, failed test name, or last 20 lines of log]

Suggest:
1. Likely cause (flaky test, env, dependency, code bug, config)—rank top 2
2. Next steps to debug (what to run locally, what to check in repo/CI)
3. Quick fix if it's a known pattern (e.g. timeout, path, secret missing)
4. How to prevent (e.g. add retry, fix env, add assertion)

Keep to one page. If log is large, paste only the failure part.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Test reporting in CI (JUnit, coverage, dashboards)</h2>
        <p>Design how to publish test results and coverage in your CI tool.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We use [e.g. GitHub Actions / Jenkins / Azure DevOps]. Our tests output: [e.g. JUnit XML, coverage (Istanbul/Cobertura), Allure]. We want: visible results in the pipeline, coverage trend, and optional badge.

Suggest:
1. Where to publish test results (action/plugin name, e.g. actions/upload-artifact, publish test results)
2. How to publish coverage (report format, where to store, trend over time)
3. Fail build if coverage drops below X% (optional)—how to configure
4. One dashboard or summary idea (e.g. PR comment with summary, SonarQube)
5. One example step or config snippet for our tool

Output as numbered list plus one snippet. No production secrets.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Green build and deployment criteria</h2>
        <p>Define when a build is "green" and when it can deploy.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have: build, unit tests, API tests, UI smoke, lint, security scan. We deploy from [e.g. main branch] to [e.g. staging then prod].

Define "green build" and "ready to deploy" criteria:
1. Green build: which jobs must pass (all? or allow known flaky to be skipped?)
2. Blocking vs non-blocking (e.g. lint can warn but not block)
3. Ready to deploy to staging: green build + what else (e.g. manual approval, no critical bugs)
4. Ready to deploy to prod: staging sign-off + what (e.g. change request, rollback plan)
5. What to do when build is red (who fixes, how long to fix, notify)

Output as short criteria list. One line per rule.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>CI/CD test environment and secrets</h2>
        <p>Suggest how to manage test env and secrets in pipelines.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Our tests need: [e.g. test DB URL, API keys for sandbox, test user credentials]. We run in [e.g. GitHub Actions / Jenkins]. We have [one shared test env / ephemeral env per run].

Suggest:
1. Where to store secrets (e.g. GitHub Secrets, Jenkins credentials, vault) and how to inject (env vars, file)
2. How to point tests at the right env (env var for base URL, config file from secret)
3. Ephemeral vs shared: pros/cons for test DB (e.g. Docker per run vs shared staging DB)
4. Cleanup: who creates/destroys test data or containers, when
5. One example: "set BASE_URL and DB_URL from secrets before running API tests" in our CI tool

Output as short sections. No real secrets.</code></pre>
        </div>
      </article>
    </section>

      <h3 class="section-heading">ISTQB AI Testing</h3>

      <article class="prompt-card">
        <h2>AI quality characteristics checklist</h2>
        <p>Map ISTQB quality characteristics to your AI feature and get one concrete test idea per characteristic.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have an AI feature: [describe in 1–2 sentences—type of model, input, output].

For each ISTQB AI quality characteristic below, suggest one concrete test idea and the metric or observable outcome to check:
- Functional correctness (accuracy, precision, recall)
- Robustness (noisy or unexpected input)
- Fairness / bias (performance across groups)
- Explainability (can we understand the decision?)
- Performance efficiency (inference latency, throughput)
- Security (prompt injection, adversarial, data poisoning)

Output as table: Characteristic | Test idea | Metric or observable. One row per characteristic.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Data quality checklist for ML datasets</h2>
        <p>Get a pre-training data quality checklist covering completeness, balance, leakage, and labels.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're about to use this dataset for [task: e.g. classification, NLP, regression]. Schema or description: [paste].

Provide a data quality checklist with 8–10 checks covering:
- Completeness (missing values, nulls)
- Correctness (label accuracy, value ranges)
- Balance (class distribution, skew)
- Uniqueness (duplicates, train/test leakage)
- Relevance (feature usefulness)
- Timeliness (data freshness)

For each check: what to verify | how (query/tool idea) | flag condition (what fails). Table.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>ML model bias test plan</h2>
        <p>Design a bias testing approach for a classification or prediction model.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have a [model type: e.g. loan approval, sentiment analysis, fraud detection] model. We want to test for bias.

Suggest:
1. Groups or slices to evaluate (protected attributes relevant to this use case)
2. Fairness metrics to calculate (demographic parity, equalized odds, etc.) and acceptable gap
3. How to structure a bias test report for stakeholders (what a "flag" looks like)
4. One concrete test: which slice to compare and which metric to use

Output as a short guide. Strategy only—no code.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Behavioral test cases for ML (CheckList style)</h2>
        <p>Generate minimum functionality, invariance, and directional behavioral tests for an ML model.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We have a [e.g. sentiment classifier / chatbot / recommendation model]. Core capability: [describe].

Generate 5 behavioral test cases:
1. Minimum functionality (basic inputs → correct output)
2. Invariance (changing irrelevant input parts doesn't change output)
3. Directional (changing one attribute changes output predictably)
4. Edge case (empty, very long, or ambiguous input)
5. Regression (specific input that must always work after retraining)

For each: input example | expected output or property | how to verify. Table.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>Adversarial and robustness test cases</h2>
        <p>Design adversarial, out-of-distribution, and safety test cases for an AI system.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're testing a [describe AI feature: model type, input type, use case].

Generate 8 adversarial/robustness test cases:
- 2 out-of-distribution (inputs model likely wasn't trained on)
- 2 boundary/edge (minimum valid, maximum valid, or ambiguous)
- 2 adversarial (slight input change that might flip output or degrade quality)
- 2 safety (inputs that might trigger harmful, biased, or incorrect output)

For each: input description | expected behavior | failure signal. No harmful content.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>ML model acceptance thresholds</h2>
        <p>Define pass/fail criteria for model performance before deployment.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>Model type: [e.g. binary classifier, multi-class, regression]. Use case: [describe business context and cost of errors].

Help define acceptance thresholds for model deployment:
1. Which metrics to use (e.g. precision, recall, F1, AUC-ROC) and why for this use case
2. Suggested threshold values with reasoning (e.g. "recall &gt;= 0.90 for fraud because false negatives are costly")
3. Bias threshold: maximum acceptable performance gap between groups
4. Latency threshold: inference time budget for this use case
5. One regression rule: model must not perform worse than current version on [key slice]

Output as a short criteria table. We'll use these as test pass/fail gates.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>AI monitoring test plan</h2>
        <p>Design monitoring tests for a deployed ML model (drift, performance, alerts).</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We're deploying an AI model to production: [describe model, input, output]. We want to monitor it.

Suggest a monitoring test plan:
1. Data drift: what to monitor (feature distributions), tool suggestions, alert threshold
2. Prediction drift: output distribution shift—what to check and when to investigate
3. Performance monitoring: if we have feedback labels, which metric to track and how often
4. Latency and error rate: thresholds and alerting
5. Retraining trigger: conditions that should trigger retraining or human review

Output as table: What to monitor | How | Threshold/alert | Action on breach.</code></pre>
        </div>
      </article>

      <article class="prompt-card">
        <h2>LLM output evaluation rubric</h2>
        <p>Get a rubric to evaluate and score LLM outputs for quality, accuracy, and safety.</p>
        <div class="prompt-block">
          <span class="prompt-label">Prompt</span>
          <pre><code>We need to evaluate outputs from our LLM feature: [describe: chatbot, summarizer, code assistant, etc.].

Create an evaluation rubric with 5–7 dimensions. For each:
- Dimension name (e.g. Accuracy, Relevance, Safety, Tone, Completeness)
- What it means for our use case
- Score 1–5 definition (1=bad, 5=excellent)
- One example of a score-1 and score-5 output

Output as table. We'll use this for human evaluation and to create test cases.</code></pre>
        </div>
      </article>

    <p class="note">Blog deep-dives: <a href="/blog/istqb-ai-testing-fundamentals">ISTQB AI Fundamentals</a>, <a href="/blog/istqb-ai-data-quality-bias">Data Quality &amp; Bias</a>, <a href="/blog/istqb-ai-model-testing">Model Testing &amp; Adversarial</a>, <a href="/blog/performance-testing-with-ai">Performance</a>, <a href="/blog/security-testing-with-ai">Security</a>, <a href="/blog/api-testing-with-ai-deep-dive">API</a>, <a href="/blog/jira-and-ai-for-testing">Jira</a>, <a href="/blog/ui-testing-with-ai">UI</a>.</p>
  </div>
</BaseLayout>

<style>
  .page h1 {
    margin: 0 0 0.5rem 0;
  }

  .lead {
    color: var(--muted);
    margin: 0 0 2rem 0;
  }

  .prompt-list {
    display: flex;
    flex-direction: column;
    gap: 2rem;
  }

  .section-heading {
    font-size: 1rem;
    font-weight: 700;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    color: var(--accent);
    margin: 2rem 0 0.5rem 0;
    padding-bottom: 0.25rem;
  }

  .section-heading:first-of-type {
    margin-top: 0;
  }

  .prompt-card h2 {
    font-size: 1.2rem;
    margin: 0 0 0.5rem 0;
  }

  .prompt-card p {
    color: var(--muted);
    margin: 0 0 1rem 0;
    font-size: 0.95rem;
  }

  .prompt-block {
    position: relative;
  }

  .prompt-label {
    position: absolute;
    top: 0.5rem;
    right: 0.75rem;
    font-size: 0.75rem;
    text-transform: uppercase;
    letter-spacing: 0.05em;
    color: var(--accent);
  }

  .prompt-block pre {
    margin: 0;
  }

  .note {
    margin-top: 2.5rem;
    padding: 1rem;
    background: var(--surface);
    border: 1px solid var(--border);
    border-radius: 8px;
    font-size: 0.9rem;
    color: var(--muted);
  }
</style>
